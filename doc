Analisis de Maquinas Disponibles

  ğŸ“Œ Especificaciones rÃ¡pidas
Recurso	Tu mÃ¡quina	RecomendaciÃ³n general JMeter
CPU	Intel i5-8500T â€“ 6 cores / 6 hilos	âœ”ï¸ Suficiente para cargas medianas
RAM	7.6 GB	âœ”ï¸ Adecuada para GUI Master o Slave de carga moderada
Disco	NVMe 218 GB	âœ”ï¸ Muy bien, rÃ¡pido y mucho espacio
Red	1 GbE estÃ¡ndar	âœ”ï¸ Apta para distribuidos
OS	Ubuntu 24.04 LTS	âœ”ï¸ Compatible


Â¿Para JMeter Master?

SÃ­, funciona bien. El Master solo coordina y recolecta resultados â†’ no ejecuta carga pesada.

RecomendaciÃ³n:

Ejecutar JMeter en modo sin interfaz (non-GUI) si tienes test grandes.

Aumentar archivos abiertos:
ulimit -n 65000


Â¿Para JMeter Slave (generaciÃ³n de carga)?

TambiÃ©n es vÃ¡lido como Slave, pero:

âœ”ï¸ Puede generar un buen nÃºmero de hilos VS (aprox. 1k-3k dependiendo del script y protocolos).
âš ï¸ No sirve para cargas muy grandes o pesadas, por:

6 hilos de CPU (sin HyperThreading)

~8 GB RAM â€” si el script usa mucha memoria, limita la carga.

ğŸ“Œ Si usas HTTP simple â†’ buena capacidad
ğŸ“Œ Si usas WebSockets / gRPC / mucha extracciÃ³n JSON o XPath â†’ reduce la carga posible


Observaciones operativas
Estado actual	Impacto	RecomendaciÃ³n
ulimit -n = 1024	âŒ LÃ­mite de conexiones bajo	Subir a 65k mÃ­nimo
Ping a Slaves 32 y 33 falla	âŒ No hay conectividad	Revisar red o firewall
RMI Port check a 1099 no responde	âŒ JMeter distribuido no funcionarÃ¡	Validar reachability y puertos
Load promedio ~1.0 idle	âœ”ï¸ OK	Sin riesgo

Ejemplo para habilitar puertos RMI:

sudo ufw allow 1099
sudo ufw allow 50000

ConclusiÃ³n
Rol	Â¿Recomendado?	Capacidad
JMeter Master	âœ… SÃ­	Ideal
JMeter Slave	âš ï¸ SÃ­, pero carga moderada	~1k-3k usuarios virtuales (variable)

Si tu objetivo es solo Master, esta mÃ¡quina es perfecta.
Si quieres Slave de alto rendimiento, necesitarÃ­as mÃ¡s CPU/RAM.


  Performance en JMeter

  | Tipo de prueba       | Usuarios Virtuales | Peticiones/seg |
| -------------------- | ------------------ | -------------- |
| HTTP simple          | **1,500 â€“ 3,000**  | **200 â€“ 800**  |
| HTTP con JSON grande | **600 â€“ 1,200**    | **80 â€“ 300**   |
| WebSockets/gRPC      | **300 â€“ 700**      | **<150**       |
| Selenium             | âŒ 5â€“20 browsers    | âŒ              |



Posibles Causas de Timeouts â€” Azure DevOps + JMeter Distribuido
1ï¸âƒ£ Problemas de Conectividad / Red

Los Slaves no responden a tiempo al Master â†’ timeout

ğŸ” SeÃ±ales:

PING fallando (como ya vimos)

nc -zv slave 1099 fallando (puertos RMI bloqueados)

SSH intermitente

ğŸ“Œ Causas tÃ­picas:

Firewall interno bloquea puertos RMI (1099, 50000)

DNS o rutas no resueltas (IPs privadas mal configuradas)

VPN o SegmentaciÃ³n de red entre Cloud y LAN

Network bandwidth saturada por respuestas grandes

2ï¸âƒ£ Recursos insuficientes del Slave

JMeter Slave saturado â†’ no procesa peticiones â†’ Master Timeout

ğŸ“Œ Signos:

CPU al 100%

GC Pauses elevadas

Heap lleno â†’ OutOfMemoryError

Hilos bloqueados esperando IO

â¡ Esto es comÃºn si el test tiene:
âœ” Assertions pesadas
âœ” JSON extractions masivos
âœ” Listeners en el slave (deben estar todos en master)

3ï¸âƒ£ El tiempo del pipeline no coincide con el SLA del test

Ejemplo:

Azure tiene timeout de job: 60 minutos

Tu test dura 65 minutos â†’ Azure lo mata automÃ¡ticamente

ğŸ“Œ Ajustar:

Timeout del Stage

Timeout del Job/Task

DuraciÃ³n real del test

4ï¸âƒ£ RMI Timeouts por configuraciÃ³n default

JMeter por defecto usa puertos dinÃ¡micos â†’ inestables en red corporativa

ğŸ“Œ Fijar puertos:

server.rmi.localport=1099
client.rmi.localport=50000
mode=StrippedBatch


ğŸ“Œ Asegurar que esos puertos estÃ¡n abiertos bidireccionalmente

5ï¸âƒ£ Azure Agent ejecutando GUI accidentalmente

El pipeline abre un runner grÃ¡fico:
â¡ Consume CPU+RAM innecesaria
â¡ Se cuelga o se desconecta

ğŸ“Œ Obligatorio correr en modo no-GUI:

jmeter -n -t plan.jmx -R slave01,slave02 ...

6ï¸âƒ£ Manejo incorrecto de resultados

Si el Master intenta guardar todos los samples en .jtl, se ahoga:

.jtl gigantescos â†’ IO Block

master tratando de â€œdigerirâ€ estadÃ­sticas de todos los slaves

ğŸ“Œ RecomendaciÃ³n:

jmeter.save.saveservice.output_format=csv
jmeter.save.saveservice.autoflush=true


Y usar Summary Report o Backend Listener (InfluxDB)

7ï¸âƒ£ GC y JVM mal configurada

Heap pequeÃ±o â†’ GC constante â†’ pÃ©rdida de respuestas

âœ” Ajustar en slaves:

export JVM_ARGS="-Xms2G -Xmx4G -XX:+UseG1GC"

ğŸ§  Resumen rÃ¡pido de las causas mÃ¡s probables en tu caso
Causa	Probabilidad	Evidencia
Red / No hay ping ni puertos accesibles	â­â­â­â­â­	Lo vimos: ping a los slaves falla
Puertos RMI bloqueados	â­â­â­â­	nc no conecta
SaturaciÃ³n del Slave	â­â­â­	Depende del script y #hilos
Timeout de Azure (Job/Task)	â­â­	Puede estar ocurriendo aparte
Logs / JTL gigantes	â­â­	Depende del listener configurado
ğŸ¯ Â¿QuÃ© hacer inmediatamente?

âœ” Probar conectividad real entre Master â†” Slaves:

ping SLAVE_IP
nc -zv SLAVE_IP 1099
nc -zv SLAVE_IP 50000


âœ” Confirmar sin GUI
âœ” Ajustar Heap, logs y sysctl como antes
âœ” Validar lÃ­mite de tiempo del pipeline

  RecomendaciÃ³n de mejora de memoria

  | Escenario de uso del Slave                          | RAM Total recomendada | Heap JVM recomendada |
| --------------------------------------------------- | --------------------- | -------------------- |
| Carga moderada (â‰¤1500 threads HTTP simples)         | **8 GB**              | `-Xms3G -Xmx4G`      |
| Carga alta (1500â€“3000 threads, JSON parsing)        | **12â€“16 GB**          | `-Xms6G -Xmx8G`      |
| Casos pesados (WebSockets, JWT, muchos extractores) | **>16 GB**            | `-Xms8G -Xmx12G`     |

Tag dinÃ¡mico por ejecuciÃ³n de prueba para aislamiento de mÃ©tricas JMeter en Datadog
1ï¸âƒ£ Problema Identificado

Actualmente, las mÃ©tricas generadas por JMeter se envÃ­an tanto a Datadog Enterprise como a Datadog CFS, mezclÃ¡ndose con las mÃ©tricas productivas del Open Engine.
Esto genera:

Dificultad para identificar el origen de las mÃ©tricas

Datos contaminados en los procesos de extracciÃ³n (API Jobs)

Riesgos de anÃ¡lisis errÃ³neo en reportes y monitoreo

No es viable modificar la configuraciÃ³n interna de Datadog.

2ï¸âƒ£ SoluciÃ³n Propuesta

Implementar un tag dinÃ¡mico por ejecuciÃ³n del test generado desde JMeter y utilizado en los pipelines de extracciÃ³n con Datadog API.

Componentes del tagging:

source:jmeter â†’ Identifica origen de las mÃ©tricas

loadtest:true â†’ Diferencia trÃ¡fico de prueba vs. trÃ¡fico real

test_id:<UUID> â†’ Etiqueta Ãºnica por ejecuciÃ³n del pipeline

Ejemplo:

source:jmeter, loadtest:true, test_id:${TEST_UUID}


El TEST_UUID se genera automÃ¡ticamente con ${__UUID} en JMeter al inicio de cada ejecuciÃ³n.







Azure DevOps Pipeline A (JMeter Test Execution)
   â”‚
   â–¼
Tagging of Metrics (test_id, source:jmeter)
   â”‚
   â”œâ”€â–º Datadog Enterprise (JMeter metrics)
   â””â”€â–º Datadog CFS (Service metrics)

â± Scheduled Pipeline Execution
Azure DevOps Pipeline B (API Job)
   â”‚
   â–¼
Python Script: Datadog Ingestion
   - Hosted on Azure DevOps Agent
   - API Keys stored in Azure Key Vault
   â”‚
   â”œâ”€â–º Datadog Enterprise API
   â”‚        (Query JMeter metrics filtered by tags)
   â”‚
   â””â”€â–º Datadog CFS API
            (Query service metrics excluding JMeter)
   â”‚
   â–¼
Python Script: Data Transformation
   - Normalize fields
   - Apply run_id and timestamps
   - Separate JMeter vs Service datasets
   â”‚
   â–¼
Python Script: Data Loading
   - Insert into Azure Database for PostgreSQL
   - Private Endpoint + Firewall rules
   â”‚
   â–¼
Azure Database for PostgreSQL (Flexible Server)
   - Central metrics repository
   - Structured tables for historical analysis
   â”‚
   â–¼
Reporting & Automation
   - Power BI (Dashboards)
   - Email & Slack Alerts (Azure DevOps or Logic Apps)
   - Export to Confluence



| Requirement                          | Recommended Azure Service                         | Alternative                            |
| ------------------------------------ | ------------------------------------------------- | -------------------------------------- |
| Scheduling of API Job                | â‡ Azure Pipelines (Scheduled Triggers)            | Azure Functions Timer (low cost)       |
| Python script execution              | âœ” Azure Pipelines Agent                           | Azure Container Apps / Azure Functions |
| Metrics persistence                  | âœ” Azure Database for PostgreSQL (Flexible Server) | CosmosDB (if scaling to NoSQL)         |
| Temporary storage for files and logs | Azure Blob Storage                                | FileShare                              |
| Secure access to Datadog API keys    | Azure Key Vault for storing API keys              | N/A                                    |


